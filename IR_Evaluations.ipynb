{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fdb2122",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ed6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b65c88",
   "metadata": {},
   "source": [
    "# Load Corpus & Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.readlines()\n",
    "\n",
    "with open('../data/queries.txt', 'r', encoding='utf-8') as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894574f",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a463b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "doc_vectors = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ffc7e",
   "metadata": {},
   "source": [
    "# Cosine Similarity for Top-10 Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_documents(query, doc_vectors, k=10):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    cosine_sim = cosine_similarity(query_vec, doc_vectors).flatten()\n",
    "    top_k_indices = cosine_sim.argsort()[-k:][::-1]\n",
    "    return top_k_indices, cosine_sim[top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f941a1",
   "metadata": {},
   "source": [
    "#  Evaluate Precision@K and MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(relevant_docs, retrieved_docs, k):\n",
    "    retrieved_at_k = retrieved_docs[:k]\n",
    "    precision = len(set(relevant_docs).intersection(set(retrieved_at_k))) / k\n",
    "    return precision\n",
    "\n",
    "def mean_average_precision(queries_relevant_docs, queries_retrieved_docs):\n",
    "    ap_scores = []\n",
    "    for rel_docs, ret_docs in zip(queries_relevant_docs, queries_retrieved_docs):\n",
    "        hits = 0\n",
    "        sum_precisions = 0\n",
    "        for i, doc in enumerate(ret_docs):\n",
    "            if doc in rel_docs:\n",
    "                hits += 1\n",
    "                sum_precisions += hits / (i + 1)\n",
    "        ap = sum_precisions / len(rel_docs) if rel_docs else 0\n",
    "        ap_scores.append(ap)\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "# Dummy relevant documents for demo (replace with actual relevance judgments)\n",
    "relevant_docs_list = [[0, 3, 5], [2, 7], [1, 4, 6]]\n",
    "\n",
    "retrieved_docs_list = []\n",
    "for q in queries:\n",
    "    top_docs, _ = get_top_k_documents(q, doc_vectors, k=10)\n",
    "    retrieved_docs_list.append(top_docs.tolist())\n",
    "\n",
    "# Compute Precision@5,6,10 and MAP\n",
    "for k in [5, 6, 10]:\n",
    "    precisions = [precision_at_k(rel, ret, k) for rel, ret in zip(relevant_docs_list, retrieved_docs_list)]\n",
    "    print(f\"Precision@{k}: {np.mean(precisions):.3f}\")\n",
    "\n",
    "map_score = mean_average_precision(relevant_docs_list, retrieved_docs_list)\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0e708",
   "metadata": {},
   "source": [
    "# Compute MRR (Mean Reciprocal Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(relevant_docs_list, retrieved_docs_list):\n",
    "    rr_scores = []\n",
    "    for rel_docs, ret_docs in zip(relevant_docs_list, retrieved_docs_list):\n",
    "        rr = 0\n",
    "        for rank, doc in enumerate(ret_docs, start=1):\n",
    "            if doc in rel_docs:\n",
    "                rr = 1 / rank\n",
    "                break\n",
    "        rr_scores.append(rr)\n",
    "    return np.mean(rr_scores)\n",
    "\n",
    "mrr = compute_mrr(relevant_docs_list, retrieved_docs_list)\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ebf4a",
   "metadata": {},
   "source": [
    "# Inter-Annotator Agreement (Cohen's Kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Annotations (Dummy Data)\n",
    "annotator1 = pd.read_csv('../annotations/annotator1.csv')  # Columns: doc_id, label\n",
    "annotator2 = pd.read_csv('../annotations/annotator2.csv')  # Columns: doc_id, label\n",
    "\n",
    "kappa = cohen_kappa_score(annotator1['label'], annotator2['label'])\n",
    "print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
    "\n",
    "# Cell 8: Discussion on Kappa Score\n",
    "if kappa >= 0.80:\n",
    "    agreement_level = \"Very Good Agreement\"\n",
    "elif 0.60 <= kappa < 0.80:\n",
    "    agreement_level = \"Substantial Agreement\"\n",
    "elif 0.40 <= kappa < 0.60:\n",
    "    agreement_level = \"Moderate Agreement\"\n",
    "else:\n",
    "    agreement_level = \"Poor Agreement\"\n",
    "\n",
    "print(f\"Agreement Level: {agreement_level}\")\n",
    "print(\"To improve Kappa, we can refine annotation guidelines, conduct calibrations, and resolve ambiguous definitions.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
